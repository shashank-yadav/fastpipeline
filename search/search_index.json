{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLPipeline Persistent, easy to use, fast to code Documentation : https://fastapi.tiangolo.com Source Code : https://github.com/shashank-yadav/mlpipeline MLPipeline is a framework for creating general purpose pipeline in your ML projects. It helps in keeping track of your experiments by automatically storing all the intermediate data and source code. The key features are: Persistence : Automatically stores all the intermediate data and variables during the run. Autoreload : Detects if something has been computed before and reloads it instead of a do-over. Accessible Intermediate Data : The intermediate data is stored as pickle and json files, can be easily accessed and analyzed. General Purpose : Unlike sklearn pipelines you don't need to format your data into the required X, y format. Intuitive : Great editor support. Completion everywhere. Less time debugging. Easy : Designed to be easy to use and learn. Less time reading docs. Installation $ pip install mlpipeline ---> 100% Example Train a classifier over the (in)famous MNIST dataset Create a file mnist_pipeline.py Make necessary imports and create a class DataLoader that extends the BaseNode class from the mlpipeline package. This is something we'll refer to as a Node # Import datasets, classifiers and performance metrics from sklearn import datasets , svm , metrics from sklearn.model_selection import train_test_split import numpy as np # Import pipeline and node constructs from mlpipeline.base_node import BaseNode from mlpipeline.pipeline import Pipeline # Node for loading data class DataLoader ( BaseNode ): def __init__ ( self ): super () . __init__ () def run ( self , input = {}): # The digits dataset digits = datasets . load_digits () # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) return { 'data' : data , 'target' : digits . target } Create another Node whose input is output of DataLoader and that trains an SVM classifier # Node for training the classifier class SVMClassifier ( BaseNode ): def __init__ ( self , config ): super () . __init__ ( config ) gamma = config [ 'gamma' ] # Create a classifier: a support vector classifier self . classifier = svm . SVC ( gamma = gamma ) def run ( self , input ): data = input [ 'data' ] target = input [ 'target' ] # Split data into train and test subsets X_train , X_test , y_train , y_test = train_test_split ( data , target , test_size = 0.5 , shuffle = False ) # We learn the digits on the first half of the digits self . classifier . fit ( X_train , y_train ) # Now predict the value of the digit on the second half: y_pred = self . classifier . predict ( X_test ) return { 'acc' : np . mean ( y_test == y_pred ), 'y_test' : y_test , 'y_pred' : y_pred } Now let's instantiate the nodes and create our pipeline if __name__ == \"__main__\" : # Initialize the nodes dl_node = DataLoader () svm_node = SVMClassifier ({ 'gamma' : 0.01 }) # Create the pipeline pipeline = Pipeline ( 'mnist' , [ dl_node , svm_node ]) # Run pipeline and see results result = pipeline . run ( input = {}) print ( 'Accuracy: %s ' % result [ 'acc' ]) Run the pipeline using $ python mnist.py . You should see somthing like: As expected it says that this is the first run and hence for both nodes outputs are being computed by calling their run method. The log here shows where the data is being stored Try running it again with the same command: $ python mnist.py . This time you should see something different: Since all the intermediate outputs are already computed, the pipeline just reloads the data at each step instead of re-computing Let's make a change to the value of config inside __main__ : # svm_node = SVMClassifier({'gamma': 0.01}) svm_node = SVMClassifier ({ 'gamma' : 0.05 }) Run the pipeline again. You'll see something like: This time it used the result from first node as-is and recomputed for second node, since we made a change to the config. If you make any changes to the class SVMClassifier same thing will happen again. To learn more about this you can look at How it works section","title":"MLPipeline"},{"location":"#mlpipeline","text":"Persistent, easy to use, fast to code Documentation : https://fastapi.tiangolo.com Source Code : https://github.com/shashank-yadav/mlpipeline MLPipeline is a framework for creating general purpose pipeline in your ML projects. It helps in keeping track of your experiments by automatically storing all the intermediate data and source code. The key features are: Persistence : Automatically stores all the intermediate data and variables during the run. Autoreload : Detects if something has been computed before and reloads it instead of a do-over. Accessible Intermediate Data : The intermediate data is stored as pickle and json files, can be easily accessed and analyzed. General Purpose : Unlike sklearn pipelines you don't need to format your data into the required X, y format. Intuitive : Great editor support. Completion everywhere. Less time debugging. Easy : Designed to be easy to use and learn. Less time reading docs.","title":"MLPipeline"},{"location":"#installation","text":"$ pip install mlpipeline ---> 100%","title":"Installation"},{"location":"#example","text":"","title":"Example"},{"location":"#train-a-classifier-over-the-infamous-mnist-dataset","text":"Create a file mnist_pipeline.py Make necessary imports and create a class DataLoader that extends the BaseNode class from the mlpipeline package. This is something we'll refer to as a Node # Import datasets, classifiers and performance metrics from sklearn import datasets , svm , metrics from sklearn.model_selection import train_test_split import numpy as np # Import pipeline and node constructs from mlpipeline.base_node import BaseNode from mlpipeline.pipeline import Pipeline # Node for loading data class DataLoader ( BaseNode ): def __init__ ( self ): super () . __init__ () def run ( self , input = {}): # The digits dataset digits = datasets . load_digits () # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) return { 'data' : data , 'target' : digits . target } Create another Node whose input is output of DataLoader and that trains an SVM classifier # Node for training the classifier class SVMClassifier ( BaseNode ): def __init__ ( self , config ): super () . __init__ ( config ) gamma = config [ 'gamma' ] # Create a classifier: a support vector classifier self . classifier = svm . SVC ( gamma = gamma ) def run ( self , input ): data = input [ 'data' ] target = input [ 'target' ] # Split data into train and test subsets X_train , X_test , y_train , y_test = train_test_split ( data , target , test_size = 0.5 , shuffle = False ) # We learn the digits on the first half of the digits self . classifier . fit ( X_train , y_train ) # Now predict the value of the digit on the second half: y_pred = self . classifier . predict ( X_test ) return { 'acc' : np . mean ( y_test == y_pred ), 'y_test' : y_test , 'y_pred' : y_pred } Now let's instantiate the nodes and create our pipeline if __name__ == \"__main__\" : # Initialize the nodes dl_node = DataLoader () svm_node = SVMClassifier ({ 'gamma' : 0.01 }) # Create the pipeline pipeline = Pipeline ( 'mnist' , [ dl_node , svm_node ]) # Run pipeline and see results result = pipeline . run ( input = {}) print ( 'Accuracy: %s ' % result [ 'acc' ]) Run the pipeline using $ python mnist.py . You should see somthing like: As expected it says that this is the first run and hence for both nodes outputs are being computed by calling their run method. The log here shows where the data is being stored Try running it again with the same command: $ python mnist.py . This time you should see something different: Since all the intermediate outputs are already computed, the pipeline just reloads the data at each step instead of re-computing Let's make a change to the value of config inside __main__ : # svm_node = SVMClassifier({'gamma': 0.01}) svm_node = SVMClassifier ({ 'gamma' : 0.05 }) Run the pipeline again. You'll see something like: This time it used the result from first node as-is and recomputed for second node, since we made a change to the config. If you make any changes to the class SVMClassifier same thing will happen again. To learn more about this you can look at How it works section","title":"Train a classifier over the (in)famous MNIST dataset"},{"location":"about/","text":"About Main Components 1. Node What A single logical unit of computation, this could include generating data, applying transformations, training models or generating results. Each node extends the BaseNode class from mlpipeline package. Following diagram shows the relevant methods and attributes of the BaseNode class: classDiagram BaseNode <|-- Node BaseNode : +config: Dict BaseNode: +__init__( config: Dict ) BaseNode: +hash( ) -> str BaseNode: +run( input: Dict ) -> Dict Why This sort of structure forces the user to experiment in the form of logical chunks each responsible for a specific task It allows allows us to uniquely identify a node run based on: config source code for the node class input to the run function Since we can identify runs uniquely, it allows us to reuse existing results from previous runs Unlike sklearn pipelines here input and output of the run method are dictionaries, allowing data other than arrays or dataframes 2. Pipeline Runs a series of nodes on an input consecutively graph LR subgraph pipeline: Node1-->Node2-->Node3 end Input:::input-->Node1 Node3-->Output:::output Node1 <--> disk[Disk Serialization]:::disk Node2 <--> disk[Disk Serialization]:::disk Node3 <--> disk[Disk Serialization]:::disk classDef input fill:#f96; classDef output fill:#99ff99; classDef disk fill:#fcf787; It is responsible for: Collecting all the node objects Calling the run method for each of the node objects Identifying based on the node and input hash if a run on same data has happened previously, if so then reusing the saved outputs Storing all the intermediate results (if not already saved): config, ouput and source code Logging for each stage The entire process (Pseudocode) For each node object within a pipeline: Convert the config dict into string and calcuate its hash (currently MD5) graph LR A[\"{'alpha': 0.03, 'gamma': 0.01 }\"]-- hash -->B[\"8d13c57118d69de715250ab3c084c66e\"] Get the source code for the object's class and calcuate its hash Compute the node_hash : hash of the concatenated string of previous two hash values graph LR A[config hash] --> C{Concatenate and hash} B[source code hash] --> C C --> D[node_hash] Create a folder by the name [node_class]_[node_hash] , for example ClassifierNode_8d13c57118d69de715250ab3c084c681 Store the config as json, source code for the node class as a python file and the node object as a pickle in the created folder Convert the input dict passed to the run method into a string and compute the input_hash Create a folder [node_class]_[node_hash]/input_[input_hash] if not already exists Check if the result from previous run exists if not then call the run method on the input dict Convert the output dict obtained from the run method into a string and compute the output_hash Store the output as [node_class]_[node_hash]/input_[input_hash]/result_[output_hash].pkl","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#main-components","text":"","title":"Main Components"},{"location":"about/#1-node","text":"","title":"1. Node"},{"location":"about/#what","text":"A single logical unit of computation, this could include generating data, applying transformations, training models or generating results. Each node extends the BaseNode class from mlpipeline package. Following diagram shows the relevant methods and attributes of the BaseNode class: classDiagram BaseNode <|-- Node BaseNode : +config: Dict BaseNode: +__init__( config: Dict ) BaseNode: +hash( ) -> str BaseNode: +run( input: Dict ) -> Dict","title":"What"},{"location":"about/#why","text":"This sort of structure forces the user to experiment in the form of logical chunks each responsible for a specific task It allows allows us to uniquely identify a node run based on: config source code for the node class input to the run function Since we can identify runs uniquely, it allows us to reuse existing results from previous runs Unlike sklearn pipelines here input and output of the run method are dictionaries, allowing data other than arrays or dataframes","title":"Why"},{"location":"about/#2-pipeline","text":"Runs a series of nodes on an input consecutively graph LR subgraph pipeline: Node1-->Node2-->Node3 end Input:::input-->Node1 Node3-->Output:::output Node1 <--> disk[Disk Serialization]:::disk Node2 <--> disk[Disk Serialization]:::disk Node3 <--> disk[Disk Serialization]:::disk classDef input fill:#f96; classDef output fill:#99ff99; classDef disk fill:#fcf787; It is responsible for: Collecting all the node objects Calling the run method for each of the node objects Identifying based on the node and input hash if a run on same data has happened previously, if so then reusing the saved outputs Storing all the intermediate results (if not already saved): config, ouput and source code Logging for each stage","title":"2. Pipeline"},{"location":"about/#the-entire-process-pseudocode","text":"For each node object within a pipeline: Convert the config dict into string and calcuate its hash (currently MD5) graph LR A[\"{'alpha': 0.03, 'gamma': 0.01 }\"]-- hash -->B[\"8d13c57118d69de715250ab3c084c66e\"] Get the source code for the object's class and calcuate its hash Compute the node_hash : hash of the concatenated string of previous two hash values graph LR A[config hash] --> C{Concatenate and hash} B[source code hash] --> C C --> D[node_hash] Create a folder by the name [node_class]_[node_hash] , for example ClassifierNode_8d13c57118d69de715250ab3c084c681 Store the config as json, source code for the node class as a python file and the node object as a pickle in the created folder Convert the input dict passed to the run method into a string and compute the input_hash Create a folder [node_class]_[node_hash]/input_[input_hash] if not already exists Check if the result from previous run exists if not then call the run method on the input dict Convert the output dict obtained from the run method into a string and compute the output_hash Store the output as [node_class]_[node_hash]/input_[input_hash]/result_[output_hash].pkl","title":"The entire process (Pseudocode)"}]}